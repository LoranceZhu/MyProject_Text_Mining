{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTXuzy_vFWWH"
      },
      "source": [
        "\n",
        "### **Prep**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "ByGh528eFWWK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /opt/anaconda3/lib/python3.8/site-packages (3.3.1)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.3.4)\n",
            "Requirement already satisfied: funcy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.17)\n",
            "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (4.1.2)\n",
            "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.24.1)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.6.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.20.1)\n",
            "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
            "Requirement already satisfied: html.parser in /opt/anaconda3/lib/python3.8/site-packages (0.2)\n",
            "Requirement already satisfied: ply in /opt/anaconda3/lib/python3.8/site-packages (from html.parser) (3.11)\n",
            "Requirement already satisfied: pattern3 in /opt/anaconda3/lib/python3.8/site-packages (3.0.0)\n",
            "Requirement already satisfied: simplejson in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (3.17.6)\n",
            "Requirement already satisfied: cherrypy in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (18.6.1)\n",
            "Requirement already satisfied: pdfminer.six in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (20220319)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (4.9.3)\n",
            "Requirement already satisfied: feedparser in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (6.0.8)\n",
            "Requirement already satisfied: pdfminer3k in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (1.3.4)\n",
            "Requirement already satisfied: docx in /opt/anaconda3/lib/python3.8/site-packages (from pattern3) (0.2.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->pattern3) (2.2.1)\n",
            "Requirement already satisfied: zc.lockfile in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (2.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (8.6.0)\n",
            "Requirement already satisfied: jaraco.collections in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (3.5.1)\n",
            "Requirement already satisfied: portend>=2.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (3.1.0)\n",
            "Requirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.8/site-packages (from cherrypy->pattern3) (8.7.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /opt/anaconda3/lib/python3.8/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (1.15.0)\n",
            "Requirement already satisfied: jaraco.functools in /opt/anaconda3/lib/python3.8/site-packages (from cheroot>=8.2.1->cherrypy->pattern3) (3.5.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /opt/anaconda3/lib/python3.8/site-packages (from portend>=2.1.1->cherrypy->pattern3) (5.0.1)\n",
            "Requirement already satisfied: pytz in /opt/anaconda3/lib/python3.8/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2021.1)\n",
            "Requirement already satisfied: Pillow>=2.0 in /opt/anaconda3/lib/python3.8/site-packages (from docx->pattern3) (8.2.0)\n",
            "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.8/site-packages (from docx->pattern3) (4.6.3)\n",
            "Requirement already satisfied: sgmllib3k in /opt/anaconda3/lib/python3.8/site-packages (from feedparser->pattern3) (1.0.0)\n",
            "Requirement already satisfied: jaraco.classes in /opt/anaconda3/lib/python3.8/site-packages (from jaraco.collections->cherrypy->pattern3) (3.2.1)\n",
            "Requirement already satisfied: jaraco.text in /opt/anaconda3/lib/python3.8/site-packages (from jaraco.collections->cherrypy->pattern3) (3.7.0)\n",
            "Requirement already satisfied: importlib-resources in /opt/anaconda3/lib/python3.8/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (5.6.0)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /opt/anaconda3/lib/python3.8/site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (4.1.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/lib/python3.8/site-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern3) (3.4.1)\n",
            "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer.six->pattern3) (4.0.0)\n",
            "Requirement already satisfied: cryptography in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer.six->pattern3) (3.4.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /opt/anaconda3/lib/python3.8/site-packages (from cryptography->pdfminer.six->pattern3) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern3) (2.20)\n",
            "Requirement already satisfied: ply in /opt/anaconda3/lib/python3.8/site-packages (from pdfminer3k->pattern3) (3.11)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from zc.lockfile->cherrypy->pattern3) (52.0.0.post20210125)\n",
            "Requirement already satisfied: pyLDAvis in /opt/anaconda3/lib/python3.8/site-packages (3.3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.11.3)\n",
            "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.20.1)\n",
            "Requirement already satisfied: funcy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.17)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
            "Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.3.4)\n",
            "Requirement already satisfied: sklearn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.6.2)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.8/site-packages (from pyLDAvis) (0.24.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/xun/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/xun/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/xun/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /Users/xun/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n",
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n",
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n",
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n",
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n",
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n",
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n",
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n",
            "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
            "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
          ]
        }
      ],
      "source": [
        "#the module 'sys' allows istalling module from inside Jupyter\n",
        "import sys\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Natrual Language ToolKit (NLTK)\n",
        "\n",
        "import nltk\n",
        "\n",
        "from sklearn import metrics\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import  CountVectorizer #bag-of-words vectorizer \n",
        "from sklearn.decomposition import LatentDirichletAllocation #package for LDA\n",
        "\n",
        "# Plotting tools\n",
        "\n",
        "from pprint import pprint\n",
        "!{sys.executable} -m pip install pyLDAvis #visualizing LDA\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#define text normalization function\n",
        "%run ./Text_Normalization_Function.ipynb #defining text normalization function\n",
        "\n",
        "#ignore warnings about future changes in functions as they take too much space\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1dg-JviFWWM"
      },
      "source": [
        "Below we define two functions that will display the results of fitting a topic model, to be used later:\n",
        "\n",
        "*Note: these functions are not the focus of the lab, therefore we'll not be discussing them, but you are welcome to explore and dig into them later if you prefer.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zs7RU66nFWWM"
      },
      "outputs": [],
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        \n",
        "def get_topic_words(vectorizer, lda_model, n_words):\n",
        "    keywords = np.array(vectorizer.get_feature_names())\n",
        "    topic_words = []\n",
        "    for topic_weights in lda_model.components_:\n",
        "        top_word_locs = (-topic_weights).argsort()[:n_words]\n",
        "        topic_words.append(keywords.take(top_word_locs).tolist())\n",
        "    return topic_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffYQm97eFWWN"
      },
      "source": [
        "Let's **normalize** our toy_corpus and call the normalized corpus **normalized_toy_corpus**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS-l-bp4FWWO"
      },
      "source": [
        "Since for topic modeling we need text data in the **Bag-of-Words** representation, let's **vectorize** our normalized_toy_corpus and call it **bow_toy_corpus**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HDwLGaPjFWWP"
      },
      "outputs": [],
      "source": [
        "#define the bag-of-words vectorizer:\n",
        "bow_vectorizer = CountVectorizer()\n",
        "\n",
        "#vectorize the normalized data:\n",
        "bow_toy_corpus = bow_vectorizer.fit_transform(normalized_toy_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxCx-R56FWWP"
      },
      "source": [
        "Have a look at the Bag-of-Words representation of our corpus: **It never hurts to know how you data look like :)** Note absence of stopwords and other differences with the raw data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbNFzhvFFWWQ"
      },
      "source": [
        "#### **Topic Model (via Latent Dirichlet Allocation) on Toy Data**\n",
        "Now let's **model topics** in our toy data. Given that the toy corpus is so small, we know all \"topics\" it contains (**what are they?**) and it will be easy for us:<br> 1) to check if the topic model results make sense; <br>2) see all the results that the topic model produces.  <br><br>\n",
        "We will be using the **LatentDirichletAllocation** function which we already imported earlier (see Session Prep). The function has the following **parameters** to be set:\n",
        "1. Number of topics to model: **n_components**\n",
        "2. Parameter vector for the Dirichlet distribution for *topics*: **doc_topic_prior**\n",
        "3. Parameter vector for the Dirichlet distribution for *words* in a topic: **topic_word_prior**\n",
        "\n",
        "Notes on **parameter vectors for the Dirichlet distributions**: <br>\n",
        "1. Although the Dirichlet distribution parameters are represented by a **vector**, for simplicity we provide one number for each parameter vector. For example, if we set the number of topics to 2 (n_components=2), the parameter vector for the Dirichlet distribution for *topics* should be a two-dimensional vector. We set doc_topic_prior=0.5 and the LatentDirichletAllocation function internally creates a two-dimensional vector (0.5,0.5). Similar logic applies to the parameter vector for the Dirichlet distribution for *words* (the dimensionality of that parameter vector is equal to the number of terms in the text corpus, which is typically very large).<br><br>\n",
        "2. Remember, that we need **sparsity** in the distribution of topics across documents (i.e., some documents have a zero probability of containing some of the topics) and *sparsity* in the distribution of words in topics (i.e., some words have zero probability to be present in some topics). To induce sparsity, we need to set **doc_topic_prior** and **topic_word_prior** between 0 and 1.\n",
        "\n",
        "Now, let's set the parameters and estimate the topic model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_7GwzdjFWWR"
      },
      "source": [
        "Display results by showing 15 **most frequent (top)** words for each topic (we use **function display_topics** defined in Session Prep):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO6T2osIFWWS"
      },
      "source": [
        "Note that topics do not have names or labels. **Topics are just collections of words**, following the definition of a topic in text mining. <br><br> To be precise, topics are **word vectors**, where each vector element is the **weight** (relative frequency) of the word in a topic. Let's have a look at those \"word vectors\". Can you see below that each word vector (topic) is a **simplex**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkcrgeBeFWWU"
      },
      "source": [
        "### **Topic Modeling on Real Data**\n",
        "\n",
        "The dataset here is the one we used for doing Text Classification in Lab 3. The newspaper blog posts have 4 topics: **atheism, religion, computer graphics, and space science**. Of course, we will *not use* class label information for topic modeling.\n",
        "\n",
        "Download the data and set up the data (**news_corpus**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          reviewer  rating  written_date  \\\n",
            "0          Kelly B     2.0  May 30, 2021   \n",
            "1              Jon     1.0  May 30, 2021   \n",
            "2          Nerdy P     2.0  May 30, 2021   \n",
            "3        ran101278     4.0  May 29, 2021   \n",
            "4  tammies20132015     5.0  May 28, 2021   \n",
            "5             John     1.0  May 28, 2021   \n",
            "6     annapN7702ZW     2.0  May 27, 2021   \n",
            "7            Deb P     2.0  May 27, 2021   \n",
            "8          Chuck N     1.0  May 27, 2021   \n",
            "9              Jen     4.0  May 26, 2021   \n",
            "\n",
            "                                               title  \\\n",
            "0  Universal is a complete Disaster - stick with ...   \n",
            "1                               Food is hard to get.   \n",
            "2                                       Disappointed   \n",
            "3                                         My opinion   \n",
            "4                  The Bourne Stuntacular...MUST SEE   \n",
            "5                             This is not a vacation   \n",
            "6                                      Expected More   \n",
            "7                                  Disapointing.....   \n",
            "8        Greed makes for a terrible guest experience   \n",
            "9                    Good first time visit with kids   \n",
            "\n",
            "                                         review_text  \\\n",
            "0  We went to Universal over Memorial Day weekend...   \n",
            "1  The food service is horrible. I’m not reviewin...   \n",
            "2  I booked this vacation mainly to ride Hagrid m...   \n",
            "3  When a person tries the test seat for the ride...   \n",
            "4  Ok, I can't stress enough to anyone and everyo...   \n",
            "5  Worst experience I have ever had the rides are...   \n",
            "6  I just expected more. Alot of waiting around, ...   \n",
            "7  This was my 4th trip with my daughter to Unive...   \n",
            "8  Universal is one thing - Not Disney. Everythin...   \n",
            "9  We spent 6 nights on site at Sapphire Falls as...   \n",
            "\n",
            "                      branch  \n",
            "0  Universal Studios Florida  \n",
            "1  Universal Studios Florida  \n",
            "2  Universal Studios Florida  \n",
            "3  Universal Studios Florida  \n",
            "4  Universal Studios Florida  \n",
            "5  Universal Studios Florida  \n",
            "6  Universal Studios Florida  \n",
            "7  Universal Studios Florida  \n",
            "8  Universal Studios Florida  \n",
            "9  Universal Studios Florida  \n"
          ]
        }
      ],
      "source": [
        "data=pd.read_csv('universal_studio_branches.csv')\n",
        "print(data.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text=data.review_text.values\n",
        "type(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#normalize data\n",
        "normalized_corpus_news = normalize_corpus(text)\n",
        "\n",
        "#define a Bag-of-Words vecgtorizer\n",
        "bow_vectorizer_news = CountVectorizer(max_features=1000)\n",
        "\n",
        "#vectorize data\n",
        "bow_news_corpus = bow_vectorizer_news.fit_transform(normalized_corpus_news)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_news = LatentDirichletAllocation(n_components=5, max_iter=500,\n",
        "                                     doc_topic_prior = 0.25,\n",
        "                                     topic_word_prior = 0.25).fit(bow_news_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic_0</th>\n",
              "      <th>Topic_1</th>\n",
              "      <th>Topic_2</th>\n",
              "      <th>Topic_3</th>\n",
              "      <th>Topic_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>park</th>\n",
              "      <td>0.021220</td>\n",
              "      <td>1.044953e-02</td>\n",
              "      <td>3.340122e-02</td>\n",
              "      <td>2.596376e-02</td>\n",
              "      <td>4.505873e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>people</th>\n",
              "      <td>0.014238</td>\n",
              "      <td>1.165629e-03</td>\n",
              "      <td>5.927128e-07</td>\n",
              "      <td>4.383927e-03</td>\n",
              "      <td>5.709567e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>universal</th>\n",
              "      <td>0.013987</td>\n",
              "      <td>9.320728e-03</td>\n",
              "      <td>1.453263e-02</td>\n",
              "      <td>2.697777e-03</td>\n",
              "      <td>3.849281e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>us</th>\n",
              "      <td>0.013453</td>\n",
              "      <td>6.812128e-03</td>\n",
              "      <td>5.923136e-07</td>\n",
              "      <td>4.835988e-03</td>\n",
              "      <td>1.686476e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>take</th>\n",
              "      <td>0.010932</td>\n",
              "      <td>1.055158e-02</td>\n",
              "      <td>2.950493e-03</td>\n",
              "      <td>5.932997e-03</td>\n",
              "      <td>2.268689e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>line</th>\n",
              "      <td>0.010359</td>\n",
              "      <td>7.447773e-07</td>\n",
              "      <td>1.398116e-03</td>\n",
              "      <td>2.662785e-02</td>\n",
              "      <td>5.607270e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>time</th>\n",
              "      <td>0.009894</td>\n",
              "      <td>8.035115e-03</td>\n",
              "      <td>6.996664e-03</td>\n",
              "      <td>3.238395e-02</td>\n",
              "      <td>2.108675e-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>0.009269</td>\n",
              "      <td>1.037181e-02</td>\n",
              "      <td>1.284914e-02</td>\n",
              "      <td>9.586760e-05</td>\n",
              "      <td>5.725668e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>walk</th>\n",
              "      <td>0.008997</td>\n",
              "      <td>9.745701e-04</td>\n",
              "      <td>3.565183e-03</td>\n",
              "      <td>9.837586e-04</td>\n",
              "      <td>3.532188e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>locker</th>\n",
              "      <td>0.008504</td>\n",
              "      <td>7.718435e-07</td>\n",
              "      <td>5.790872e-07</td>\n",
              "      <td>4.567965e-07</td>\n",
              "      <td>5.430756e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>could</th>\n",
              "      <td>0.008179</td>\n",
              "      <td>4.707958e-04</td>\n",
              "      <td>2.246532e-03</td>\n",
              "      <td>3.560905e-03</td>\n",
              "      <td>2.697085e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>staff</th>\n",
              "      <td>0.007631</td>\n",
              "      <td>7.485864e-07</td>\n",
              "      <td>5.772440e-07</td>\n",
              "      <td>4.473993e-07</td>\n",
              "      <td>5.126014e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>night</th>\n",
              "      <td>0.007234</td>\n",
              "      <td>7.491823e-07</td>\n",
              "      <td>5.131722e-04</td>\n",
              "      <td>4.485442e-07</td>\n",
              "      <td>1.187036e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>want</th>\n",
              "      <td>0.006912</td>\n",
              "      <td>1.593478e-03</td>\n",
              "      <td>1.583096e-03</td>\n",
              "      <td>6.086892e-03</td>\n",
              "      <td>2.042647e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ticket</th>\n",
              "      <td>0.006909</td>\n",
              "      <td>7.550365e-07</td>\n",
              "      <td>5.867262e-07</td>\n",
              "      <td>2.128382e-02</td>\n",
              "      <td>5.605173e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Topic_0       Topic_1       Topic_2       Topic_3       Topic_4\n",
              "park       0.021220  1.044953e-02  3.340122e-02  2.596376e-02  4.505873e-02\n",
              "people     0.014238  1.165629e-03  5.927128e-07  4.383927e-03  5.709567e-07\n",
              "universal  0.013987  9.320728e-03  1.453263e-02  2.697777e-03  3.849281e-02\n",
              "us         0.013453  6.812128e-03  5.923136e-07  4.835988e-03  1.686476e-03\n",
              "take       0.010932  1.055158e-02  2.950493e-03  5.932997e-03  2.268689e-03\n",
              "line       0.010359  7.447773e-07  1.398116e-03  2.662785e-02  5.607270e-07\n",
              "time       0.009894  8.035115e-03  6.996664e-03  3.238395e-02  2.108675e-02\n",
              "like       0.009269  1.037181e-02  1.284914e-02  9.586760e-05  5.725668e-03\n",
              "walk       0.008997  9.745701e-04  3.565183e-03  9.837586e-04  3.532188e-03\n",
              "locker     0.008504  7.718435e-07  5.790872e-07  4.567965e-07  5.430756e-07\n",
              "could      0.008179  4.707958e-04  2.246532e-03  3.560905e-03  2.697085e-03\n",
              "staff      0.007631  7.485864e-07  5.772440e-07  4.473993e-07  5.126014e-03\n",
              "night      0.007234  7.491823e-07  5.131722e-04  4.485442e-07  1.187036e-03\n",
              "want       0.006912  1.593478e-03  1.583096e-03  6.086892e-03  2.042647e-03\n",
              "ticket     0.006909  7.550365e-07  5.867262e-07  2.128382e-02  5.605173e-07"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_weights = lda_news.components_ / lda_news.components_.sum(axis=1)[:, np.newaxis]\n",
        "word_weights_df = pd.DataFrame(word_weights.T, \n",
        "                               index = bow_vectorizer_news.get_feature_names(), \n",
        "                               columns = [\"Topic_\" + str(i) for i in range(5)])\n",
        "\n",
        "word_weights_df.sort_values(by='Topic_0',ascending=False).head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el50511404809437874725950216417\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el50511404809437874725950216417_data = {\"mdsDat\": {\"x\": [150.22215270996094, 51.16336441040039, -73.1445541381836, -66.4101333618164, 54.41732406616211], \"y\": [-72.2695541381836, -159.968017578125, 21.523815155029297, -110.60643768310547, 12.44344425201416], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [25.03567293564824, 20.68852548167343, 19.927865157200973, 19.330236803124592, 15.017699622352746]}, \"tinfo\": {\"Term\": [\"potter\", \"harry\", \"pas\", \"ride\", \"express\", \"wait\", \"show\", \"line\", \"queue\", \"ticket\", \"transformer\", \"great\", \"place\", \"day\", \"singapore\", \"universal\", \"coaster\", \"hour\", \"buy\", \"long\", \"mummy\", \"minute\", \"roller\", \"love\", \"visit\", \"fun\", \"studio\", \"people\", \"simpson\", \"year\", \"pas\", \"min\", \"unlimited\", \"peak\", \"timed\", \"monday\", \"lane\", \"popular\", \"express\", \"hr\", \"online\", \"priority\", \"single\", \"9am\", \"rider\", \"per\", \"buy\", \"thursday\", \"queuing\", \"extra\", \"entry\", \"normal\", \"faster\", \"save\", \"sunday\", \"early\", \"friday\", \"tip\", \"afford\", \"bottle\", \"wait\", \"hour\", \"queue\", \"long\", \"minute\", \"ticket\", \"line\", \"pass\", \"purchase\", \"fast\", \"cost\", \"arrive\", \"busy\", \"day\", \"time\", \"crowd\", \"expensive\", \"ride\", \"worth\", \"pay\", \"park\", \"spend\", \"want\", \"food\", \"take\", \"visit\", \"attraction\", \"good\", \"locker\", \"service\", \"house\", \"halloween\", \"guest\", \"japanese\", \"usj\", \"room\", \"customer\", \"employee\", \"understand\", \"someone\", \"phone\", \"sign\", \"guide\", \"exit\", \"english\", \"speak\", \"item\", \"security\", \"rude\", \"scare\", \"haunt\", \"attendant\", \"push\", \"explain\", \"shuttle\", \"write\", \"suppose\", \"rent\", \"care\", \"call\", \"tour\", \"night\", \"bus\", \"event\", \"people\", \"bag\", \"know\", \"horror\", \"review\", \"leave\", \"us\", \"let\", \"staff\", \"give\", \"put\", \"walk\", \"could\", \"work\", \"take\", \"hotel\", \"way\", \"park\", \"front\", \"universal\", \"want\", \"like\", \"line\", \"pay\", \"even\", \"think\", \"time\", \"free\", \"look\", \"ticket\", \"experience\", \"use\", \"around\", \"disney\", \"first\", \"thing\", \"wait\", \"gras\", \"mardi\", \"refillable\", \"clean\", \"age\", \"california\", \"variety\", \"yr\", \"florida\", \"friendly\", \"citywalk\", \"orlando\", \"teenager\", \"teen\", \"kingdom\", \"family\", \"memory\", \"perfect\", \"old\", \"maintain\", \"atmosphere\", \"plenty\", \"young\", \"blast\", \"adult\", \"year\", \"selection\", \"child\", \"wonderful\", \"range\", \"vacation\", \"great\", \"place\", \"disney\", \"universal\", \"love\", \"visit\", \"studio\", \"kid\", \"always\", \"fun\", \"food\", \"enjoy\", \"theme\", \"lot\", \"park\", \"day\", \"much\", \"good\", \"well\", \"time\", \"ride\", \"attraction\", \"spend\", \"really\", \"recommend\", \"experience\", \"potter\", \"harry\", \"simpson\", \"alley\", \"diagon\", \"hogwarts\", \"gringotts\", \"wand\", \"black\", \"men\", \"wizarding\", \"hulk\", \"spiderman\", \"escape\", \"jaw\", \"et\", \"despicable\", \"terminator\", \"hp\", \"forbidden\", \"simulator\", \"rocket\", \"butter\", \"jimmy\", \"hogsmeade\", \"butterbeer\", \"twister\", \"spider\", \"fallon\", \"rockit\", \"minion\", \"motion\", \"beer\", \"fan\", \"train\", \"journey\", \"new\", \"ride\", \"rip\", \"world\", \"island\", \"adventure\", \"area\", \"park\", \"like\", \"coaster\", \"3d\", \"love\", \"really\", \"universal\", \"great\", \"fun\", \"movie\", \"good\", \"mummy\", \"show\", \"transformer\", \"time\", \"well\", \"much\", \"studio\", \"singapore\", \"sentosa\", \"madagascar\", \"battlestar\", \"galactica\", \"waterworld\", \"action\", \"rapid\", \"human\", \"cylon\", \"sesame\", \"egypt\", \"performance\", \"sgd\", \"fi\", \"sci\", \"ancient\", \"dance\", \"canopy\", \"donkey\", \"boot\", \"flyer\", \"stunt\", \"pus\", \"spielberg\", \"battle\", \"adrenaline\", \"thrilling\", \"lost\", \"firework\", \"transformer\", \"show\", \"jurassic\", \"roller\", \"street\", \"mummy\", \"coaster\", \"4d\", \"wet\", \"camera\", \"shrek\", \"revenge\", \"ride\", \"miss\", \"water\", \"movie\", \"character\", \"must\", \"place\", \"far\", \"studio\", \"enjoy\", \"best\", \"kid\", \"visit\", \"take\", \"world\", \"attraction\", \"like\", \"fun\", \"good\", \"theme\", \"universal\", \"park\", \"time\", \"day\", \"queue\", \"us\"], \"Freq\": [20281.0, 20172.0, 18915.0, 99402.0, 19443.0, 24810.0, 15103.0, 21314.0, 14761.0, 15864.0, 10493.0, 22440.0, 15194.0, 39610.0, 5946.0, 36265.0, 9187.0, 11425.0, 10456.0, 14620.0, 7972.0, 10337.0, 7331.0, 13393.0, 20544.0, 16310.0, 17805.0, 9902.0, 4644.0, 10034.0, 18914.747713871107, 4120.846773733832, 1520.5358542723293, 1125.9879801439863, 377.8081689176927, 723.372146100656, 632.7077626837408, 1638.1542650924166, 18211.22452839119, 551.8282258313145, 1605.447122704923, 456.1905000864033, 3086.2854824748106, 340.08075345324875, 2156.588429609156, 2277.7027511209335, 9502.27153798965, 404.9020349902033, 329.90661497457467, 2845.727234778341, 2244.5439218790393, 857.2729984399471, 360.6125552837926, 2546.4023227298453, 424.22315562503894, 5691.57504178199, 539.0062406432999, 1721.8278804687086, 449.61098061179916, 888.461317727579, 20608.738530113667, 9540.638932443766, 12257.67046617568, 11979.27108735691, 8522.324612818933, 12508.457931198443, 15649.130945354109, 5521.160827634793, 3314.4498099600987, 5704.684102133807, 2720.669645100184, 2033.1425390012994, 2758.315336137972, 19464.91521963875, 19031.97731853385, 4307.8972837306765, 3923.205387147782, 30017.76559651296, 5707.557337441211, 3740.584605691579, 15258.849369034238, 3571.4254576982953, 3577.253861317966, 3492.8553769853397, 3486.809753283487, 3640.1687394189453, 3408.3273042990686, 3559.5177123314666, 4129.981971724854, 2972.0483171013366, 2156.1670650602337, 2044.4849497368034, 2016.314873970324, 1711.4588791652427, 1624.906863456422, 1536.4213842481422, 1418.740441889157, 1360.3789703068453, 1014.2838654401977, 890.543483867101, 848.2983581063928, 822.1314190983668, 765.8082404594857, 779.8705674890754, 765.7955570748637, 754.7425945411578, 765.7841323431662, 742.6725482311966, 709.4832935262756, 677.2887190536836, 590.777820524278, 508.2632101650643, 503.2257856478098, 458.96497104513924, 463.98005072118434, 443.85936612595896, 444.85909099654265, 437.81503594736023, 726.91582882035, 1215.256754887519, 1692.2004163911583, 3513.0814018507576, 930.9005452991237, 1136.8081413700136, 6914.721976373634, 1369.209321951417, 3152.6312619163905, 1752.8091581572658, 1520.157087318744, 2876.990135701246, 6533.655316319502, 1849.709147775027, 3706.2451070559946, 3155.2452419194756, 1745.1199134355086, 4369.195718588943, 3972.011310378226, 2061.314072791094, 5309.08136567874, 2334.6612374725514, 3047.165579045677, 10305.53673834501, 1860.7967121643005, 6792.915821598142, 3356.689022640753, 4501.674268570085, 5030.634087803, 2707.191878425083, 3262.6954882303485, 2675.6423343665388, 4804.869856411732, 2223.6645531933923, 2363.8709251518976, 3355.5541656108185, 2998.099087481324, 2466.097855131627, 2647.629578967787, 2636.1985424023974, 2525.5536798097983, 2463.170047769856, 2425.763326527252, 413.62776317447987, 404.7249885921968, 412.5828203782671, 2276.7618483298097, 3558.356369314215, 352.2725067109555, 757.2758052104491, 379.61939270820466, 2057.4347918860967, 1958.9938823025866, 396.102534711944, 3548.327464664418, 677.1768016713338, 662.2714093360695, 339.657319518563, 6711.601934597738, 427.99787921434614, 746.4490358662267, 4974.348523955094, 277.61833302182765, 749.9118317696935, 1981.516009633683, 2449.0702460304465, 854.8367157595688, 2742.061734471202, 6606.1927934063515, 265.4281476249128, 3698.690523396783, 1155.2558288024952, 332.4460130678194, 887.2925283308357, 13534.477169458729, 8891.827468673124, 5807.796729325114, 18006.748218131932, 7266.246869789299, 10430.06704265853, 8912.80766804219, 6806.333365164551, 2127.849702025387, 7911.419299821923, 6252.968189874466, 5869.385384975786, 4471.791926570671, 5638.338245847514, 21078.252102792077, 13773.267596465801, 5678.955003653478, 7165.086401298119, 5116.194411620601, 9864.277962358925, 12177.358803997855, 4372.954109962966, 3410.9972348068436, 4009.1142858424123, 3110.0456028270405, 3293.9236444312546, 20280.545998977126, 20171.27951836394, 4643.513850603178, 4134.284614892594, 3650.1084926081126, 3276.1845655840452, 2549.427515872007, 2429.124833721662, 1952.9632142365674, 1888.8091642257514, 1627.1787844429246, 1562.0022467476315, 1574.0206144175859, 1543.9590622366773, 1547.9647920041184, 1498.8659228510655, 1436.6990315933317, 1417.651428440255, 1338.4647646313913, 1314.4138602450416, 1292.3424193256105, 1148.0070132856779, 1109.9167120484356, 1090.8745247975176, 1028.7278308436792, 1027.7178586674047, 1025.709812220945, 1008.65179861642, 814.2092589375111, 819.2091167109601, 2990.9244609656953, 1145.4883623372746, 1544.912019194419, 2988.1793634195515, 2447.978616958616, 1440.7584097549036, 4427.028560170168, 31568.84952804716, 2018.1470840109168, 5038.072699627347, 4344.3612318253445, 4600.245393540084, 4469.854907990985, 15156.339087202592, 5830.506363526096, 3770.252435335052, 2358.7315954651094, 4459.797761039141, 4510.012984536426, 6594.413860030043, 5138.726202635815, 4128.882974131093, 2548.1905311434984, 4158.0611049922745, 2884.1637115658677, 3065.1271870622427, 2825.2848204092716, 3174.848935920612, 2747.295180165477, 2603.155130946773, 2568.2118056447225, 5945.591287188861, 2094.6858701301694, 1603.7801552539954, 1536.376591780009, 1485.0723491300596, 1258.7200785828727, 1234.5489203016512, 949.878525117686, 943.8456732408556, 730.5832209449298, 694.3623367743961, 675.257802649151, 607.8098737591071, 579.6758256785666, 545.4791357983257, 526.3745724834041, 481.0979269773323, 468.98527816092906, 448.90686372889553, 437.83320066179766, 428.7701683442247, 423.7394822153976, 422.732615284153, 406.65870447384026, 393.5768470152533, 388.5153460036341, 379.4600157632815, 381.46082374618396, 588.6046934626471, 661.947391776727, 7667.666323833116, 10282.848326997155, 2889.5027667165423, 4948.907357138843, 1905.7713173685222, 5087.582966814246, 5416.603478080499, 2286.496030118932, 1458.6906184498011, 880.387456939119, 2371.073835078523, 1335.2468512243026, 23428.006163672024, 2380.429227872817, 3128.903737096834, 2513.4113979839603, 2000.124672041113, 3176.6098736811546, 4801.395770149787, 1784.4141625370316, 4653.074161668315, 3735.2608673992713, 3024.4513831336426, 3627.7410042625893, 4375.461464381816, 3719.767290270194, 2892.2519964961184, 3395.3486623051544, 3656.3922689632277, 3576.7587953605475, 3732.8283497137368, 2795.508273402393, 3285.853017399556, 3683.792141328281, 2832.633409050601, 2731.569744567617, 2503.027450481901, 2401.4918398782484], \"Total\": [20281.0, 20172.0, 18915.0, 99402.0, 19443.0, 24810.0, 15103.0, 21314.0, 14761.0, 15864.0, 10493.0, 22440.0, 15194.0, 39610.0, 5946.0, 36265.0, 9187.0, 11425.0, 10456.0, 14620.0, 7972.0, 10337.0, 7331.0, 13393.0, 20544.0, 16310.0, 17805.0, 9902.0, 4644.0, 10034.0, 18915.80728004671, 4121.903881674264, 1521.5880235530942, 1127.0575213993163, 378.8439281778806, 731.6000278616732, 646.9739323407382, 1724.3572012428817, 19443.42228228455, 589.4151077241041, 1719.952909318861, 492.76513310135255, 3335.244780713174, 371.1646729422217, 2355.7010671393055, 2496.072598837173, 10456.733325273726, 450.0074461072984, 374.2799848629395, 3242.0250817829665, 2561.789649448302, 988.8416470105652, 419.2535278663395, 2973.323670132165, 497.1021641921829, 6696.323045173335, 636.8244181129221, 2034.62416448022, 533.4535502739104, 1054.9350272407414, 24810.37585698697, 11425.024009395367, 14761.490208777208, 14620.262670861704, 10337.400438419476, 15864.806714872157, 21314.707551277057, 7140.709656438311, 4237.4410419364185, 7722.834133968477, 3578.7937582779887, 2609.1300855890418, 3746.9247020001276, 39610.334572617205, 39708.60748227572, 6791.680701396184, 6063.9574256272645, 99402.11923733685, 11556.81384060568, 6448.56870442949, 65482.7694387022, 8323.898958931994, 9169.58987828408, 13064.856733103028, 14915.773834282636, 20544.10602255233, 13912.807366108173, 20412.800444537294, 4131.039347669528, 2973.0885155339415, 2157.191147541799, 2045.520692575905, 2017.3516172663706, 1712.5216524539733, 1626.0020792541839, 1537.47099967791, 1419.764578826695, 1361.4141857311317, 1015.3367774123028, 891.5940196724204, 849.3404481219202, 823.1833637041743, 766.845276041913, 780.9297663890741, 766.8451874831455, 755.7788360271207, 766.8451740396386, 743.706353881304, 710.5071661533319, 678.3139626701794, 591.7946266677534, 509.29931667357374, 504.2690718441342, 460.0034181638853, 465.03341412837506, 444.91272136080477, 445.91870323453907, 438.8764718710512, 738.6377473243086, 1274.89169557656, 1886.5561198479913, 4301.758822537382, 1014.0359754742198, 1270.8635135534928, 9902.603928321294, 1575.7022577076993, 4235.50177001819, 2135.6987511144816, 1855.1857234456331, 4166.634112218716, 12566.440884937385, 2442.830631519946, 6104.9577985767555, 5058.4066053813185, 2307.739700106343, 8561.014530905994, 8511.801356435037, 3216.7060618249834, 14915.773834282636, 4084.8935787230193, 6491.928139160242, 65482.7694387022, 2848.742806334407, 36265.408615389926, 9169.58987828408, 16723.353508330965, 21314.707551277057, 6448.56870442949, 10159.621438751721, 7088.722832857717, 39708.60748227572, 4193.916042375087, 5154.765872773733, 15864.806714872157, 11688.367769886741, 6107.7026587936, 8545.48101498879, 9863.24390491414, 8698.726706183266, 7851.994597766953, 24810.37585698697, 414.67468271848134, 405.77322355298503, 413.68627361081155, 2441.0327829306375, 4200.683307217275, 421.7645093742811, 913.7960027221227, 484.5389652879499, 2631.0812582011745, 2521.9430707446863, 523.1752968863882, 4877.831388553421, 934.7677448162369, 921.0117074722074, 472.89342029221564, 9359.784022143032, 601.358318995195, 1059.155057191888, 7097.553681735704, 403.7346433008485, 1102.1790450692235, 2932.202371873347, 3640.2412718856376, 1276.1049751310404, 4110.745795059976, 10034.710736773905, 404.5405728735905, 5769.680405947272, 1805.297200578206, 523.4865160784344, 1409.5606877693497, 22440.75129724284, 15194.908892338994, 9863.24390491414, 36265.408615389926, 13393.592868402991, 20544.10602255233, 17805.21755308926, 13358.437537885431, 3569.607283061441, 16310.428641462646, 13064.856733103028, 13226.502913154463, 9518.103121545357, 12686.389503719936, 65482.7694387022, 39610.334572617205, 12809.802737861473, 20412.800444537294, 13052.131626097293, 39708.60748227572, 99402.11923733685, 13912.807366108173, 8323.898958931994, 14592.046271640616, 6710.111713263615, 11688.367769886741, 20281.583535681635, 20172.31773456199, 4644.545043941991, 4135.306338580337, 3651.1285307866656, 3277.2189122977625, 2550.451044110579, 2430.1584332085085, 1954.0001263228125, 1889.8440618983434, 1628.2075632556716, 1563.0489547589164, 1575.07825421524, 1545.0052029754368, 1549.0148828770116, 1499.8954221376457, 1437.7441968185587, 1418.6979232094307, 1339.5052652697111, 1315.446773862764, 1293.3930727404638, 1149.0419537115736, 1110.9493048074921, 1091.9029516746232, 1029.7518064633314, 1028.7493436631191, 1026.7444483915015, 1009.7030439043572, 815.2299406123019, 820.2421355567171, 3368.7666425126067, 1202.3680274369242, 1707.9799852051835, 3765.6142630485633, 3126.4094822838238, 1591.6455772216482, 6988.412446124843, 99402.11923733685, 2519.1597097230947, 9372.067182772564, 7762.592609450624, 8531.48187133815, 8245.801930314508, 65482.7694387022, 16723.353508330965, 9187.645206174291, 3733.1580112977836, 13393.592868402991, 14592.046271640616, 36265.408615389926, 22440.75129724284, 16310.428641462646, 5062.399928717066, 20412.800444537294, 7972.532216716099, 15103.413990199824, 10493.739114407292, 39708.60748227572, 13052.131626097293, 12809.802737861473, 17805.21755308926, 5946.621524750403, 2095.7106514330844, 1604.7900902210479, 1537.3890690926783, 1486.0838395000344, 1259.737158809154, 1235.5933818073236, 950.8997204195359, 944.8638318584569, 731.5950210388439, 695.3795243540601, 676.2658815190861, 608.8646326990959, 580.697168122212, 546.4937625709848, 527.3800980062094, 482.11073843171965, 470.03868572729385, 449.9191918120174, 438.85331725561207, 429.7994343034374, 424.7694733785074, 423.76348924640905, 407.6678405277258, 394.5900213555641, 389.55993801144433, 380.5059405859861, 382.5178123475636, 617.9201033272426, 750.7129872325497, 10493.739114407292, 15103.413990199824, 3901.7134166666788, 7331.485100062938, 2509.2021782318675, 7972.532216716099, 9187.645206174291, 3326.378922710729, 1924.1971791841336, 1039.4347969129185, 3603.350158558558, 1794.3066326152389, 99402.11923733685, 4346.213119812258, 6479.888938949743, 5062.399928717066, 3648.9699614607953, 7441.716320501927, 15194.908892338994, 3318.3341857847763, 17805.21755308926, 13226.502913154463, 9269.27850938089, 13358.437537885431, 20544.10602255233, 14915.773834282636, 9372.067182772564, 13912.807366108173, 16723.353508330965, 16310.428641462646, 20412.800444537294, 9518.103121545357, 36265.408615389926, 65482.7694387022, 39708.60748227572, 39610.334572617205, 14761.490208777208, 12566.440884937385], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.4363, -4.9602, -5.9572, -6.2576, -7.3496, -6.7, -6.834, -5.8826, -3.4742, -6.9707, -5.9028, -7.1611, -5.2492, -7.4548, -5.6077, -5.553, -4.1247, -7.2803, -7.4852, -5.3304, -5.5677, -6.5302, -7.3962, -5.4415, -7.2337, -4.6372, -6.9942, -5.8328, -7.1756, -6.4945, -3.3505, -4.1207, -3.8701, -3.893, -4.2335, -3.8498, -3.6258, -4.6676, -5.1779, -4.6349, -5.3753, -5.6666, -5.3616, -3.4076, -3.4301, -4.9158, -5.0093, -2.9744, -4.6344, -5.057, -3.6511, -5.1032, -5.1016, -5.1255, -5.1272, -5.0842, -5.15, -5.1066, -4.7672, -5.0962, -5.4172, -5.4703, -5.4842, -5.6481, -5.7, -5.756, -5.8357, -5.8777, -6.1713, -6.3014, -6.35, -6.3813, -6.4523, -6.4341, -6.4523, -6.4669, -6.4523, -6.483, -6.5287, -6.5751, -6.7118, -6.8622, -6.8722, -6.9643, -6.9534, -6.9977, -6.9955, -7.0114, -6.5044, -5.9905, -5.6595, -4.929, -6.2571, -6.0573, -4.2518, -5.8713, -5.0373, -5.6243, -5.7667, -5.1287, -4.3085, -5.5705, -4.8755, -5.0364, -5.6287, -4.7109, -4.8062, -5.4621, -4.5161, -5.3376, -5.0713, -3.8528, -5.5645, -4.2696, -4.9745, -4.681, -4.5699, -5.1896, -5.0029, -5.2013, -4.6159, -5.3863, -5.3252, -4.9749, -5.0875, -5.2829, -5.2118, -5.2162, -5.259, -5.284, -5.2993, -7.0308, -7.0526, -7.0333, -5.3253, -4.8787, -7.1914, -6.4261, -7.1166, -5.4266, -5.4756, -7.0741, -4.8816, -6.5379, -6.5601, -7.2278, -4.2442, -6.9967, -6.4405, -4.5437, -7.4295, -6.4358, -5.4642, -5.2523, -6.3049, -5.1393, -4.26, -7.4744, -4.8401, -6.0037, -7.2493, -6.2676, -3.5428, -3.9629, -4.3888, -3.2573, -4.1648, -3.8033, -3.9605, -4.2302, -5.3929, -4.0797, -4.315, -4.3783, -4.6502, -4.4184, -3.0998, -3.5253, -4.4113, -4.1788, -4.5156, -3.8591, -3.6485, -4.6726, -4.921, -4.7595, -5.0134, -4.956, -3.1079, -3.1133, -4.5821, -4.6983, -4.8228, -4.9309, -5.1817, -5.2301, -5.4482, -5.4816, -5.6307, -5.6716, -5.6639, -5.6832, -5.6806, -5.7129, -5.7552, -5.7686, -5.8261, -5.8442, -5.8611, -5.9796, -6.0133, -6.0306, -6.0893, -6.0902, -6.0922, -6.109, -6.3231, -6.317, -5.022, -5.9818, -5.6826, -5.0229, -5.2223, -5.7524, -4.6299, -2.6654, -5.4154, -4.5006, -4.6487, -4.5915, -4.6202, -3.3992, -4.3545, -4.7904, -5.2595, -4.6225, -4.6113, -4.2314, -4.4808, -4.6996, -5.1822, -4.6925, -5.0583, -4.9975, -5.079, -4.9623, -5.107, -5.1609, -5.1744, -4.0825, -5.1257, -5.3928, -5.4357, -5.4697, -5.635, -5.6544, -5.9166, -5.9229, -6.1791, -6.2299, -6.2578, -6.363, -6.4104, -6.4712, -6.5069, -6.5968, -6.6223, -6.6661, -6.6911, -6.712, -6.7238, -6.7262, -6.7649, -6.7976, -6.8106, -6.8341, -6.8289, -6.3951, -6.2777, -3.8281, -3.5347, -4.8041, -4.266, -5.2203, -4.2383, -4.1757, -5.0381, -5.4876, -5.9925, -5.0018, -5.576, -2.7112, -4.9979, -4.7245, -4.9435, -5.1719, -4.7093, -4.2962, -5.2861, -4.3276, -4.5473, -4.7584, -4.5765, -4.3891, -4.5515, -4.8031, -4.6427, -4.5687, -4.5907, -4.548, -4.8371, -4.6755, -4.5612, -4.8239, -4.8603, -4.9476, -4.9891], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3848, 1.3846, 1.3842, 1.3839, 1.3821, 1.3736, 1.3626, 1.3336, 1.3194, 1.319, 1.316, 1.3077, 1.3073, 1.2974, 1.2966, 1.2933, 1.2892, 1.2792, 1.2587, 1.2545, 1.2527, 1.2421, 1.2342, 1.2299, 1.2263, 1.2223, 1.2181, 1.2179, 1.2139, 1.2131, 1.1993, 1.2046, 1.199, 1.1856, 1.1918, 1.1472, 1.0759, 1.1276, 1.1392, 1.082, 1.1107, 1.1354, 1.0786, 0.6744, 0.6494, 0.9296, 0.9494, 0.1875, 0.6794, 0.8403, -0.0718, 0.5387, 0.4436, 0.0657, -0.0686, -0.3457, -0.0217, -0.3617, 1.5753, 1.5752, 1.5751, 1.5751, 1.5751, 1.575, 1.5749, 1.5749, 1.5749, 1.5748, 1.5746, 1.5744, 1.5744, 1.5743, 1.5742, 1.5742, 1.5742, 1.5742, 1.5742, 1.5742, 1.5741, 1.5741, 1.5739, 1.5736, 1.5735, 1.5733, 1.5733, 1.5732, 1.5732, 1.5732, 1.5596, 1.5277, 1.4669, 1.3731, 1.49, 1.4641, 1.2164, 1.4351, 1.2803, 1.378, 1.3764, 1.2052, 0.9215, 1.2975, 1.0765, 1.1036, 1.2961, 0.903, 0.8134, 1.1306, 0.5426, 1.0162, 0.8192, -0.2735, 1.1497, -0.0994, 0.5707, 0.2632, 0.1317, 0.7076, 0.4397, 0.6013, -0.5363, 0.9411, 0.796, 0.0221, 0.215, 0.6687, 0.4039, 0.2561, 0.3389, 0.4163, -0.7495, 1.6105, 1.6105, 1.6104, 1.5434, 1.4471, 1.433, 1.4252, 1.369, 1.3671, 1.3605, 1.3348, 1.2948, 1.2907, 1.2833, 1.2821, 1.2805, 1.273, 1.2632, 1.2576, 1.2385, 1.228, 1.2212, 1.2167, 1.2124, 1.2082, 1.195, 1.1916, 1.1684, 1.1666, 1.159, 1.1502, 1.1074, 1.0772, 1.0834, 0.9129, 1.0015, 0.9352, 0.921, 0.9388, 1.0957, 0.8896, 0.8762, 0.8006, 0.8576, 0.8021, 0.4795, 0.5567, 0.7996, 0.5661, 0.6765, 0.2204, -0.4865, 0.4557, 0.7209, 0.3211, 0.8441, 0.3465, 1.6434, 1.6434, 1.6433, 1.6433, 1.6432, 1.6432, 1.6431, 1.6431, 1.643, 1.643, 1.6429, 1.6428, 1.6428, 1.6428, 1.6428, 1.6428, 1.6428, 1.6428, 1.6427, 1.6427, 1.6427, 1.6426, 1.6426, 1.6426, 1.6425, 1.6425, 1.6425, 1.6425, 1.6422, 1.6422, 1.5245, 1.595, 1.5432, 1.4123, 1.3989, 1.5439, 1.187, 0.4965, 1.4218, 1.0228, 1.0631, 1.0258, 1.0312, 0.1801, 0.5898, 0.7528, 1.1844, 0.5438, 0.4693, -0.0611, 0.1694, 0.2697, 0.957, 0.0524, 0.6267, 0.0487, 0.3313, -0.8828, 0.0852, 0.05, -0.2928, 1.8958, 1.8955, 1.8953, 1.8953, 1.8953, 1.8951, 1.8951, 1.8949, 1.8949, 1.8946, 1.8945, 1.8944, 1.8942, 1.8942, 1.8941, 1.894, 1.8938, 1.8937, 1.8937, 1.8936, 1.8935, 1.8935, 1.8935, 1.8935, 1.8934, 1.8933, 1.8932, 1.8932, 1.8473, 1.7701, 1.5822, 1.5115, 1.5956, 1.5029, 1.6209, 1.4467, 1.3675, 1.5211, 1.619, 1.7299, 1.4774, 1.6004, 0.4507, 1.2939, 1.1679, 1.1957, 1.2947, 1.0447, 0.7439, 1.2756, 0.554, 0.6315, 0.776, 0.5924, 0.3494, 0.5072, 0.7202, 0.4855, 0.3756, 0.3786, 0.1969, 0.6708, -0.5053, -0.9819, -0.7444, -0.7783, 0.1214, 0.241]}, \"token.table\": {\"Topic\": [4, 5, 4, 5, 1, 2, 5, 5, 1, 2, 3, 5, 3, 4, 5, 1, 2, 3, 3, 5, 4, 1, 2, 3, 4, 5, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 5, 5, 2, 4, 1, 3, 4, 5, 4, 3, 4, 5, 5, 1, 2, 2, 4, 1, 3, 4, 4, 4, 1, 2, 4, 2, 3, 2, 5, 2, 5, 5, 2, 4, 2, 3, 4, 5, 2, 3, 5, 3, 4, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 5, 1, 2, 3, 4, 5, 4, 4, 2, 3, 4, 5, 1, 2, 4, 5, 2, 2, 1, 3, 4, 5, 1, 2, 4, 4, 1, 2, 3, 4, 5, 2, 5, 2, 1, 3, 1, 2, 3, 4, 5, 2, 1, 4, 1, 2, 4, 1, 2, 3, 5, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 5, 2, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 5, 4, 1, 2, 3, 1, 2, 2, 3, 1, 2, 1, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 3, 4, 5, 4, 2, 2, 2, 4, 2, 4, 4, 2, 4, 1, 2, 3, 4, 1, 2, 2, 4, 1, 2, 4, 5, 3, 4, 5, 2, 2, 4, 4, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 5, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 4, 2, 1, 2, 4, 5, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 3, 4, 5, 5, 1, 2, 3, 5, 3, 2, 3, 4, 5, 4, 1, 4, 5, 1, 2, 1, 2, 3, 4, 5, 1, 2, 4, 5, 4, 5, 1, 2, 3, 4, 5, 4, 5, 1, 3, 4, 5, 2, 3, 4, 5, 2, 3, 4, 1, 2, 2, 3, 4, 5, 1, 2, 2, 3, 4, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 1, 1, 2, 5, 1, 2, 3, 4, 5, 5, 2, 1, 2, 3, 5, 1, 3, 4, 5, 1, 4, 4, 1, 2, 1, 2, 5, 2, 2, 4, 5, 1, 5, 1, 5, 1, 3, 5, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 3, 2, 4, 5, 2, 4, 1, 2, 3, 4, 5, 1, 4, 2, 4, 4, 4, 4, 5, 2, 2, 1, 2, 2, 5, 2, 1, 3, 4, 5, 5, 2, 5, 5, 2, 4, 5, 4, 5, 2, 2, 4, 4, 5, 1, 2, 2, 2, 1, 2, 3, 4, 5, 4, 4, 5, 2, 3, 2, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 1, 2, 1, 2, 3, 4, 5, 1, 1, 2, 4, 2, 5, 1, 2, 4, 4, 5, 4, 2, 1, 2, 3, 4, 5, 1, 1, 2, 3, 5, 1, 2, 4, 2, 2, 3, 3, 5, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 5, 4, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 3, 4, 5, 2, 2, 3, 4, 2, 3, 5, 3, 4, 5], \"Freq\": [0.6319046750394378, 0.36805299851809564, 0.3123516665242994, 0.6872337917945606, 0.9160354548422418, 0.0808266577801978, 0.9995197596425649, 0.9960422678719105, 0.08295331723255457, 0.020677513093158763, 0.6670322458993098, 0.22915549804418303, 0.30545689943443, 0.5391794848036754, 0.1553071342097543, 0.8435598559029932, 0.08810514050542373, 0.06935936592980166, 0.8470050560314631, 0.15283227823839218, 0.9996841011346247, 0.07535842984085762, 0.1064542875075312, 0.5961440100421748, 0.16668500280784493, 0.055468286648661, 0.9976960927372561, 0.23951581867849575, 0.12745879768663243, 0.5420940301229752, 0.09083410034946497, 0.280265089326062, 0.309871380599337, 0.13340384210092301, 0.10578690636774948, 0.17061649279223312, 0.7791869064822907, 0.2203799661718235, 0.6804702043241037, 0.223194227018306, 0.09617312221113998, 0.9974488152034837, 0.24495415700945763, 0.056279080087560246, 0.3143147090969361, 0.14037425722989166, 0.24401976615232063, 0.13073535878515827, 0.8688189620236975, 0.9985626396433304, 0.9990964752380489, 0.09484888663993248, 0.9045773448067634, 0.09375055449251377, 0.326131100380747, 0.25384931498375707, 0.3262389836425335, 0.9994881646580572, 0.6700075751308797, 0.2648684916891665, 0.06504167103609709, 0.9981399828858941, 0.8417580012701142, 0.15735566239959342, 0.9181133830726395, 0.0808649811084387, 0.7360703028079976, 0.1870867593431495, 0.07659614826174595, 0.9991455012362993, 0.9992715974326157, 0.9086967893724368, 0.06837699478018232, 0.022856086367081918, 0.16359840258339095, 0.8345889523094726, 0.953022130597945, 0.04627844090969445, 0.15200568661858727, 0.8466139507870684, 0.9979569846569215, 0.9842443100606948, 0.014892279794590981, 0.10852377634850932, 0.21512920311510053, 0.12798132210796426, 0.5480998805480268, 0.16482715386102287, 0.6411100337875116, 0.19394488451155056, 0.7569164720825774, 0.2408370592990019, 0.0667750147150038, 0.9328018926752372, 0.4103336508321502, 0.5895961237553733, 0.7603120447234897, 0.1992291392458097, 0.04051644486766116, 0.2458938962923123, 0.4666462284152243, 0.14826473823263167, 0.11971613966644348, 0.019502334822992757, 0.6343054376973866, 0.12044146890354276, 0.11690773387458796, 0.0768587368797669, 0.05153363583892416, 0.9994614749247183, 0.9991866797589751, 0.9977902122552175, 0.4914121582163115, 0.04183251714176359, 0.34771228641732643, 0.050087938448557, 0.06897189911363796, 0.9994823857956057, 0.9996909090498596, 0.2672548732863305, 0.588852922627848, 0.14386747541475833, 0.9980555752410661, 0.8500187284277982, 0.03464587930345207, 0.11528715009596982, 0.9981281304385154, 0.998961237699773, 0.9988978381856715, 0.09601933393421153, 0.44373029201565944, 0.1778247822151697, 0.2823875686962835, 0.8763404912981342, 0.12335126737203136, 0.9993493853784434, 0.9994030102869642, 0.2859358508102963, 0.3211733842320127, 0.13346953999957376, 0.16969136206435484, 0.08966869538319447, 0.8946672777006606, 0.1046532523607633, 0.9988094110007187, 0.6469372597209815, 0.35290485235862873, 0.051076421597383126, 0.2564943248726208, 0.28181864780867677, 0.21294675604001106, 0.19763238507530154, 0.9978186723744565, 0.9366149505785591, 0.06331189963001517, 0.8778463855791113, 0.12183742877854849, 0.998491296073623, 0.12126348186185266, 0.08280105589685975, 0.7171105641028679, 0.07884797322823547, 0.045410917862192805, 0.0863073000304834, 0.7934960384341059, 0.07435705848780108, 0.17810141080176656, 0.1645403896747285, 0.11963834194298024, 0.5376191486807979, 0.738718442094575, 0.036644578284446054, 0.039234301131403373, 0.185424155842144, 0.8610541736814888, 0.1383411137770813, 0.9972666429641257, 0.11722189637934169, 0.881828356853684, 0.30004391311026574, 0.29038732740096984, 0.13197333802704408, 0.1510565907382717, 0.12657024840398565, 0.12884436728942705, 0.7818078569744881, 0.08931689177880638, 0.9981884918132482, 0.26735846181532363, 0.1133575384908372, 0.4786122134930486, 0.14060621080868868, 0.99890016541033, 0.425854972287084, 0.5302919699700308, 0.04387307665219679, 0.8463871432524501, 0.15231827995452257, 0.22284404692531426, 0.776782006986994, 0.34646862391554856, 0.6532706272612319, 0.04248815375938856, 0.4850271059026305, 0.2531509190079587, 0.21930754112169248, 0.9992706740553756, 0.20876139116151488, 0.6237141942372911, 0.010279916989013989, 0.05713261557355852, 0.10022919064288639, 0.17440037243653642, 0.08803299698552133, 0.3510052439628605, 0.20369571589638158, 0.18287544671505349, 0.9983729830959095, 0.07651258985303926, 0.603099237665133, 0.2290030281041169, 0.09135166522931303, 0.9994310637273631, 0.99933000412283, 0.9988977228284225, 0.9992565743375639, 0.9999346760952643, 0.9986572594073256, 0.9992699148876335, 0.999628065036123, 0.8208086459221947, 0.17886417726313655, 0.1904578381312962, 0.5716183188130805, 0.19657794861109362, 0.041371946843430665, 0.8350967133332902, 0.1649011851923193, 0.9994478247589897, 0.9988762528160664, 0.9365216343561767, 0.06277409505648286, 0.9993289047308962, 0.9990857604775094, 0.382732953985365, 0.5596068502566225, 0.05758385406646185, 0.9988978556972768, 0.9991114550570541, 0.9993448204479956, 0.999173047684102, 0.9053523099755583, 0.09424208639579025, 0.09098566760018076, 0.16813126181892557, 0.7407002235620349, 0.0944721264310203, 0.03338713818402144, 0.5094907230503362, 0.09110346899092846, 0.2715886487256273, 0.7189780728814188, 0.23261055299104724, 0.04863675198903715, 0.08499583273659073, 0.7444218350513071, 0.059024883844854666, 0.09207881879797328, 0.01936016190111233, 0.9784010890667869, 0.02009354527309357, 0.28656223893011806, 0.6904853948090031, 0.022800178139331002, 0.19690272170065204, 0.7573181603871232, 0.045439089623227395, 0.0033486106702284827, 0.26920437923872553, 0.16013534597985493, 0.34867408603754074, 0.21861643947063095, 0.7341878823508606, 0.2360342025757032, 0.029744719624924636, 0.9997484052844681, 0.8193423243943653, 0.06648307365483956, 0.04144932369838762, 0.07270731203199017, 0.45860472780851114, 0.1656719278969833, 0.2615055723713507, 0.11406919625693931, 0.04693163378865175, 0.9531976655695131, 0.1778283726302499, 0.027825095540105593, 0.4444132823091086, 0.16978826003792477, 0.1800354623614764, 0.5424981983095305, 0.332995040525806, 0.1244624960889055, 0.9995076675598495, 0.049537487881852936, 0.1114593477341691, 0.6885710815577558, 0.15108933803965144, 0.9980944441177892, 0.058201572830124365, 0.711722090608378, 0.0881338102856169, 0.14300957895401986, 0.999553369552885, 0.9997807125784076, 0.8878620330226115, 0.111910393329831, 0.8243852069740427, 0.17547932004821798, 0.017256401822108473, 0.06810526585792144, 0.0779989362359303, 0.2889872091809099, 0.5476031511549089, 0.9882449049560462, 0.009568069619214832, 0.9522874642972543, 0.04657475807916702, 0.5033185911579539, 0.49640487424644353, 0.18501455865476182, 0.09211695325426959, 0.443332353839828, 0.20320375366174895, 0.0763477799005726, 0.36174203146562195, 0.6381912122389336, 0.07672961120902917, 0.2617675702892974, 0.2346232945200787, 0.4269176441525143, 0.018172939988741362, 0.1921752630305484, 0.6334772073240789, 0.15611557108438445, 0.8166427140440815, 0.12901699581396675, 0.054163891936313976, 0.866670616666334, 0.13247823895366367, 0.04917187183776991, 0.7008048438999069, 0.2117631042182469, 0.038182169822451706, 0.9331650833601108, 0.06628088442557796, 0.09307414788165508, 0.7273724156037714, 0.17938299426530438, 0.2330231315931713, 0.1573849134411969, 0.32188620274728785, 0.23145019873032993, 0.05625907443405486, 0.9999573224639712, 0.7731724528278607, 0.14368319808030885, 0.04747427304992661, 0.03557069426159693, 0.580128734215133, 0.41978307498539563, 0.9990616970480767, 0.26013359906606787, 0.6983011791699764, 0.04150423494415912, 0.9126337114798806, 0.0873372032935092, 0.7043350215197495, 0.18505316919285644, 0.10952126339985381, 0.9985799262222492, 0.9984217775982714, 0.0304707322222535, 0.06831235431252512, 0.5851960063072962, 0.3159610915745984, 0.124138771420284, 0.6759424311950628, 0.024213881238571877, 0.17563589912485236, 0.9499191923920187, 0.049293730984933816, 0.9999712282977999, 0.9253901491163526, 0.07305711703550152, 0.7820757780940296, 0.21758414828083744, 0.9983618022779004, 0.9974833438832702, 0.7561511378079548, 0.16336331172126017, 0.08016502034067144, 0.8304039650896067, 0.1695628262864485, 0.881692885930957, 0.11755905145746093, 0.240693878696049, 0.6342092676753037, 0.12225720822656458, 0.9990538219748987, 0.12068218310289162, 0.13384003611581338, 0.2747387121291837, 0.3090724848347764, 0.16166341279938748, 0.37599970131836774, 0.463479615973097, 0.07317314837388766, 0.0873308858393038, 0.9983410771529316, 0.9980029189823857, 0.25525180126679664, 0.7440199884086758, 0.8193249768960634, 0.18003588308110866, 0.3019855132899904, 0.022232926389862042, 0.12250241839337109, 0.31758880235364473, 0.2356891400279131, 0.9156509839422867, 0.08405141159970922, 0.1984788809022989, 0.8010607633216784, 0.999093197852169, 0.9984856477095339, 0.32490006696999923, 0.6750337663453091, 0.999043234195495, 0.9978787460209703, 0.8562808097803997, 0.1432740082350551, 0.998062898978215, 0.9973831056359029, 0.9990502247592512, 0.10134953759708928, 0.6550640844689917, 0.14584445654215286, 0.09640565771430444, 0.999660901931954, 0.9996338771858778, 0.9980161562057187, 0.9987994290992215, 0.11619889391489696, 0.20293425062630152, 0.6808394450865444, 0.3416265269352659, 0.6579987777120353, 0.997777763711212, 0.9985624543007655, 0.9998826485830508, 0.9989229316517738, 0.9998954827126938, 0.9252694188581031, 0.07435736094517485, 0.9993337554320535, 0.9989694921450635, 0.4290056880337458, 0.05237929991114902, 0.40978392659846175, 0.044450323319094355, 0.06427276479923102, 0.9993037122066716, 0.9993154281621537, 0.9985047230704488, 0.6070476033206941, 0.39279550803103735, 0.12832764246478548, 0.11158925431720476, 0.759603995459258, 0.025105000748639777, 0.0687438946674163, 0.5005836055316025, 0.14422738685124598, 0.26132789369892817, 0.9981983128188632, 0.8529433797356771, 0.14483944184190745, 0.9979397517352937, 0.23377935591819093, 0.3559319187180028, 0.07113274924840854, 0.0897707363276334, 0.249400402642865, 0.10640472776287403, 0.01954372550746666, 0.7187747936634961, 0.1552640415315407, 0.1583280989537084, 0.03316331802408757, 0.7242440742679769, 0.027814395762137963, 0.055628791524275925, 0.9995080536892225, 0.04339099868177783, 0.0819491016266022, 0.4698415159925192, 0.11115660194992964, 0.29375601044612787, 0.0944982820302779, 0.3136782596234157, 0.27865531143160116, 0.20593493536786975, 0.10723389955457412, 0.18437735976102507, 0.3775010059070414, 0.19622716712133578, 0.1814149079209474, 0.06051865901872973, 0.9960320479241251, 0.8999851080318192, 0.09777615988493839, 0.7884117483936705, 0.211537402271279, 0.47929154928172935, 0.12100650978870899, 0.24840961759746627, 0.07995747525060375, 0.0713447330346332, 0.997772359235267, 0.8463479546061101, 0.14154948369718914, 0.011795790308099095, 0.896872339072708, 0.10283288048469583, 0.061412300943939416, 0.15544988676434665, 0.7830068370352276, 0.26920814108304253, 0.7307214250707151, 0.9992749428617143, 0.9986834147624302, 0.0437055602160615, 0.18731348299539796, 0.4965337683347757, 0.18182616029319215, 0.09060976080124801, 0.9996135461478455, 0.22615790946874462, 0.5199562915090701, 0.06278627395173804, 0.19106444075807735, 0.48414275631814563, 0.4037524643491874, 0.11198973463700089, 0.9993837158839037, 0.36961870781490797, 0.629274076452636, 0.8284124659606298, 0.1707164394846212, 0.1771797709768526, 0.009783828012732795, 0.5076881899144431, 0.09233791910524435, 0.21295645550102477, 0.8306605316580159, 0.09778167061974606, 0.07154264853670621, 0.06751536256752873, 0.5103367111722025, 0.19296778367051462, 0.18899629175477764, 0.04018215350039772, 0.9995233095946839, 0.39009378254432575, 0.36610143360394226, 0.10425766175912088, 0.07830230245088786, 0.06128954592952504, 0.28580736760284214, 0.13487885490544496, 0.05540218410875828, 0.04120440990818513, 0.48287864645210216, 0.9994148312574579, 0.2338288359729687, 0.46935208380081395, 0.09488706387308875, 0.1725219343147068, 0.029421151298311608, 0.14365469593112296, 0.13461402706719094, 0.39196662633793333, 0.21046370651882387, 0.11929085950120451, 0.24165922548392968, 0.7582383010345235, 0.9992583480860038, 0.07145637846149847, 0.6397838536669049, 0.1634079972569151, 0.12463321824679965, 0.14984272443175597, 0.640717541605496, 0.09108696734129565, 0.10041327799057506, 0.017719990233630894, 0.039479017039071404, 0.11438244936725554, 0.5375548320076804, 0.30857653318106626, 0.4939077568200103, 0.2607119956725107, 0.16483782003190603, 0.08055853566913623, 0.9979485384054357, 0.23000164733617495, 0.6583149403391559, 0.1116125844958908, 0.07664327146521213, 0.6727576050835287, 0.2505328443594031, 0.7842506531423641, 0.07636124780596704, 0.1382757730540484], \"Term\": [\"3d\", \"3d\", \"4d\", \"4d\", \"9am\", \"9am\", \"action\", \"adrenaline\", \"adult\", \"adult\", \"adult\", \"adult\", \"adventure\", \"adventure\", \"adventure\", \"afford\", \"afford\", \"afford\", \"age\", \"age\", \"alley\", \"always\", \"always\", \"always\", \"always\", \"always\", \"ancient\", \"area\", \"area\", \"area\", \"area\", \"around\", \"around\", \"around\", \"around\", \"around\", \"arrive\", \"arrive\", \"atmosphere\", \"atmosphere\", \"atmosphere\", \"attendant\", \"attraction\", \"attraction\", \"attraction\", \"attraction\", \"attraction\", \"bag\", \"bag\", \"battle\", \"battlestar\", \"beer\", \"beer\", \"best\", \"best\", \"best\", \"best\", \"black\", \"blast\", \"blast\", \"blast\", \"boot\", \"bottle\", \"bottle\", \"bus\", \"bus\", \"busy\", \"busy\", \"busy\", \"butter\", \"butterbeer\", \"buy\", \"buy\", \"buy\", \"california\", \"california\", \"call\", \"call\", \"camera\", \"camera\", \"canopy\", \"care\", \"care\", \"character\", \"character\", \"character\", \"character\", \"child\", \"child\", \"child\", \"citywalk\", \"citywalk\", \"clean\", \"clean\", \"coaster\", \"coaster\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"could\", \"could\", \"crowd\", \"crowd\", \"crowd\", \"crowd\", \"crowd\", \"customer\", \"cylon\", \"dance\", \"day\", \"day\", \"day\", \"day\", \"day\", \"despicable\", \"diagon\", \"disney\", \"disney\", \"disney\", \"donkey\", \"early\", \"early\", \"early\", \"egypt\", \"employee\", \"english\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"entry\", \"entry\", \"escape\", \"et\", \"even\", \"even\", \"even\", \"even\", \"even\", \"event\", \"event\", \"exit\", \"expensive\", \"expensive\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"explain\", \"express\", \"express\", \"extra\", \"extra\", \"fallon\", \"family\", \"family\", \"family\", \"family\", \"fan\", \"fan\", \"fan\", \"fan\", \"far\", \"far\", \"far\", \"far\", \"fast\", \"fast\", \"fast\", \"fast\", \"faster\", \"faster\", \"fi\", \"firework\", \"firework\", \"first\", \"first\", \"first\", \"first\", \"first\", \"florida\", \"florida\", \"florida\", \"flyer\", \"food\", \"food\", \"food\", \"food\", \"forbidden\", \"free\", \"free\", \"free\", \"friday\", \"friday\", \"friendly\", \"friendly\", \"front\", \"front\", \"fun\", \"fun\", \"fun\", \"fun\", \"galactica\", \"give\", \"give\", \"give\", \"give\", \"give\", \"good\", \"good\", \"good\", \"good\", \"good\", \"gras\", \"great\", \"great\", \"great\", \"great\", \"gringotts\", \"guest\", \"guide\", \"halloween\", \"harry\", \"haunt\", \"hogsmeade\", \"hogwarts\", \"horror\", \"horror\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"hour\", \"hour\", \"house\", \"hp\", \"hr\", \"hr\", \"hulk\", \"human\", \"island\", \"island\", \"island\", \"item\", \"japanese\", \"jaw\", \"jimmy\", \"journey\", \"journey\", \"jurassic\", \"jurassic\", \"jurassic\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kingdom\", \"kingdom\", \"kingdom\", \"know\", \"know\", \"know\", \"know\", \"know\", \"lane\", \"lane\", \"leave\", \"leave\", \"leave\", \"let\", \"let\", \"let\", \"like\", \"like\", \"like\", \"like\", \"like\", \"line\", \"line\", \"line\", \"locker\", \"long\", \"long\", \"long\", \"long\", \"look\", \"look\", \"look\", \"look\", \"lost\", \"lost\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"love\", \"love\", \"love\", \"madagascar\", \"maintain\", \"maintain\", \"maintain\", \"maintain\", \"mardi\", \"memory\", \"memory\", \"memory\", \"memory\", \"men\", \"min\", \"minion\", \"minion\", \"minute\", \"minute\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"monday\", \"monday\", \"motion\", \"motion\", \"movie\", \"movie\", \"much\", \"much\", \"much\", \"much\", \"much\", \"mummy\", \"mummy\", \"must\", \"must\", \"must\", \"must\", \"new\", \"new\", \"new\", \"new\", \"night\", \"night\", \"night\", \"normal\", \"normal\", \"old\", \"old\", \"old\", \"old\", \"online\", \"online\", \"orlando\", \"orlando\", \"orlando\", \"park\", \"park\", \"park\", \"park\", \"park\", \"pas\", \"pass\", \"pass\", \"pass\", \"pass\", \"pay\", \"pay\", \"peak\", \"people\", \"people\", \"people\", \"per\", \"per\", \"perfect\", \"perfect\", \"perfect\", \"performance\", \"phone\", \"place\", \"place\", \"place\", \"place\", \"plenty\", \"plenty\", \"plenty\", \"plenty\", \"popular\", \"popular\", \"potter\", \"priority\", \"priority\", \"purchase\", \"purchase\", \"pus\", \"push\", \"put\", \"put\", \"put\", \"queue\", \"queue\", \"queuing\", \"queuing\", \"range\", \"range\", \"range\", \"rapid\", \"really\", \"really\", \"really\", \"really\", \"really\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"refillable\", \"rent\", \"revenge\", \"revenge\", \"review\", \"review\", \"ride\", \"ride\", \"ride\", \"ride\", \"ride\", \"rider\", \"rider\", \"rip\", \"rip\", \"rocket\", \"rockit\", \"roller\", \"roller\", \"room\", \"rude\", \"save\", \"save\", \"scare\", \"sci\", \"security\", \"selection\", \"selection\", \"selection\", \"selection\", \"sentosa\", \"service\", \"sesame\", \"sgd\", \"show\", \"show\", \"show\", \"shrek\", \"shrek\", \"shuttle\", \"sign\", \"simpson\", \"simulator\", \"singapore\", \"single\", \"single\", \"someone\", \"speak\", \"spend\", \"spend\", \"spend\", \"spend\", \"spend\", \"spider\", \"spiderman\", \"spielberg\", \"staff\", \"staff\", \"street\", \"street\", \"street\", \"studio\", \"studio\", \"studio\", \"studio\", \"studio\", \"stunt\", \"sunday\", \"sunday\", \"suppose\", \"take\", \"take\", \"take\", \"take\", \"take\", \"teen\", \"teen\", \"teen\", \"teen\", \"teenager\", \"teenager\", \"teenager\", \"teenager\", \"teenager\", \"terminator\", \"theme\", \"theme\", \"theme\", \"theme\", \"theme\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thrilling\", \"thursday\", \"thursday\", \"ticket\", \"ticket\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timed\", \"tip\", \"tip\", \"tip\", \"tour\", \"tour\", \"train\", \"train\", \"train\", \"transformer\", \"transformer\", \"twister\", \"understand\", \"universal\", \"universal\", \"universal\", \"universal\", \"universal\", \"unlimited\", \"us\", \"us\", \"us\", \"us\", \"use\", \"use\", \"use\", \"usj\", \"vacation\", \"vacation\", \"variety\", \"variety\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"wait\", \"wait\", \"wait\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"wand\", \"want\", \"want\", \"want\", \"want\", \"want\", \"water\", \"water\", \"water\", \"water\", \"water\", \"waterworld\", \"way\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wet\", \"wet\", \"wizarding\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"work\", \"work\", \"work\", \"work\", \"work\", \"world\", \"world\", \"world\", \"world\", \"worth\", \"worth\", \"worth\", \"worth\", \"write\", \"year\", \"year\", \"year\", \"young\", \"young\", \"young\", \"yr\", \"yr\", \"yr\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 1, 5, 3, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el50511404809437874725950216417\", ldavis_el50511404809437874725950216417_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el50511404809437874725950216417\", ldavis_el50511404809437874725950216417_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el50511404809437874725950216417\", ldavis_el50511404809437874725950216417_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "PreparedData(topic_coordinates=                x           y  topics  cluster       Freq\n",
              "topic                                                    \n",
              "3      150.222153  -72.269554       1        1  25.035673\n",
              "0       51.163364 -159.968018       2        1  20.688525\n",
              "4      -73.144554   21.523815       3        1  19.927865\n",
              "2      -66.410133 -110.606438       4        1  19.330237\n",
              "1       54.417324   12.443444       5        1  15.017700, topic_info=        Term          Freq         Total Category  logprob  loglift\n",
              "651   potter  20281.000000  20281.000000  Default  30.0000  30.0000\n",
              "386    harry  20172.000000  20172.000000  Default  29.0000  29.0000\n",
              "616      pas  18915.000000  18915.000000  Default  28.0000  28.0000\n",
              "715     ride  99402.000000  99402.000000  Default  27.0000  27.0000\n",
              "295  express  19443.000000  19443.000000  Default  26.0000  26.0000\n",
              "..       ...           ...           ...      ...      ...      ...\n",
              "610     park   3683.792141  65482.769439   Topic5  -4.5612  -0.9819\n",
              "890     time   2832.633409  39708.607482   Topic5  -4.8239  -0.7444\n",
              "201      day   2731.569745  39610.334573   Topic5  -4.8603  -0.7783\n",
              "674    queue   2503.027450  14761.490209   Topic5  -4.9476   0.1214\n",
              "927       us   2401.491840  12566.440885   Topic5  -4.9891   0.2410\n",
              "\n",
              "[347 rows x 6 columns], token_table=      Topic      Freq   Term\n",
              "term                        \n",
              "3         4  0.631905     3d\n",
              "3         5  0.368053     3d\n",
              "5         4  0.312352     4d\n",
              "5         5  0.687234     4d\n",
              "7         1  0.916035    9am\n",
              "...     ...       ...    ...\n",
              "996       3  0.672758  young\n",
              "996       5  0.250533  young\n",
              "998       3  0.784251     yr\n",
              "998       4  0.076361     yr\n",
              "998       5  0.138276     yr\n",
              "\n",
              "[626 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 1, 5, 3, 2])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#prepare to display result in the Jupyter notebook\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "#run the visualization [mds is a function to use for visualizing the \"distance\" between topics]\n",
        "pyLDAvis.sklearn.prepare(lda_news, bow_news_corpus, bow_vectorizer_news, mds='tsne')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_news_topic_weights = lda_news.transform(bow_news_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic_0</th>\n",
              "      <th>Topic_1</th>\n",
              "      <th>Topic_2</th>\n",
              "      <th>Topic_3</th>\n",
              "      <th>Topic_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc_0</th>\n",
              "      <td>0.51723</td>\n",
              "      <td>0.00247</td>\n",
              "      <td>0.02920</td>\n",
              "      <td>0.43227</td>\n",
              "      <td>0.01883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_1</th>\n",
              "      <td>0.52954</td>\n",
              "      <td>0.01627</td>\n",
              "      <td>0.01566</td>\n",
              "      <td>0.29845</td>\n",
              "      <td>0.14008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_2</th>\n",
              "      <td>0.75160</td>\n",
              "      <td>0.00332</td>\n",
              "      <td>0.00345</td>\n",
              "      <td>0.17388</td>\n",
              "      <td>0.06775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_3</th>\n",
              "      <td>0.57943</td>\n",
              "      <td>0.01716</td>\n",
              "      <td>0.01608</td>\n",
              "      <td>0.37172</td>\n",
              "      <td>0.01561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_4</th>\n",
              "      <td>0.22183</td>\n",
              "      <td>0.14848</td>\n",
              "      <td>0.00753</td>\n",
              "      <td>0.20130</td>\n",
              "      <td>0.42085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_5</th>\n",
              "      <td>0.19180</td>\n",
              "      <td>0.01456</td>\n",
              "      <td>0.01451</td>\n",
              "      <td>0.16501</td>\n",
              "      <td>0.61412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_6</th>\n",
              "      <td>0.66917</td>\n",
              "      <td>0.00859</td>\n",
              "      <td>0.00848</td>\n",
              "      <td>0.17895</td>\n",
              "      <td>0.13482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_7</th>\n",
              "      <td>0.52788</td>\n",
              "      <td>0.00320</td>\n",
              "      <td>0.05528</td>\n",
              "      <td>0.24934</td>\n",
              "      <td>0.16430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_8</th>\n",
              "      <td>0.59171</td>\n",
              "      <td>0.00284</td>\n",
              "      <td>0.10940</td>\n",
              "      <td>0.29313</td>\n",
              "      <td>0.00292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_9</th>\n",
              "      <td>0.40790</td>\n",
              "      <td>0.00073</td>\n",
              "      <td>0.17579</td>\n",
              "      <td>0.32029</td>\n",
              "      <td>0.09528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_10</th>\n",
              "      <td>0.93163</td>\n",
              "      <td>0.01693</td>\n",
              "      <td>0.01662</td>\n",
              "      <td>0.01819</td>\n",
              "      <td>0.01663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_11</th>\n",
              "      <td>0.01794</td>\n",
              "      <td>0.01823</td>\n",
              "      <td>0.01840</td>\n",
              "      <td>0.29181</td>\n",
              "      <td>0.65362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_12</th>\n",
              "      <td>0.52685</td>\n",
              "      <td>0.00094</td>\n",
              "      <td>0.16374</td>\n",
              "      <td>0.30748</td>\n",
              "      <td>0.00099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_13</th>\n",
              "      <td>0.98855</td>\n",
              "      <td>0.00281</td>\n",
              "      <td>0.00280</td>\n",
              "      <td>0.00301</td>\n",
              "      <td>0.00283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_14</th>\n",
              "      <td>0.02104</td>\n",
              "      <td>0.71531</td>\n",
              "      <td>0.02118</td>\n",
              "      <td>0.02065</td>\n",
              "      <td>0.22183</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Topic_0  Topic_1  Topic_2  Topic_3  Topic_4\n",
              "Doc_0   0.51723  0.00247  0.02920  0.43227  0.01883\n",
              "Doc_1   0.52954  0.01627  0.01566  0.29845  0.14008\n",
              "Doc_2   0.75160  0.00332  0.00345  0.17388  0.06775\n",
              "Doc_3   0.57943  0.01716  0.01608  0.37172  0.01561\n",
              "Doc_4   0.22183  0.14848  0.00753  0.20130  0.42085\n",
              "Doc_5   0.19180  0.01456  0.01451  0.16501  0.61412\n",
              "Doc_6   0.66917  0.00859  0.00848  0.17895  0.13482\n",
              "Doc_7   0.52788  0.00320  0.05528  0.24934  0.16430\n",
              "Doc_8   0.59171  0.00284  0.10940  0.29313  0.00292\n",
              "Doc_9   0.40790  0.00073  0.17579  0.32029  0.09528\n",
              "Doc_10  0.93163  0.01693  0.01662  0.01819  0.01663\n",
              "Doc_11  0.01794  0.01823  0.01840  0.29181  0.65362\n",
              "Doc_12  0.52685  0.00094  0.16374  0.30748  0.00099\n",
              "Doc_13  0.98855  0.00281  0.00280  0.00301  0.00283\n",
              "Doc_14  0.02104  0.71531  0.02118  0.02065  0.22183"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#array of document \"names\" and topic \"names\" (\"names\" are just indecies)\n",
        "doc_names = [\"Doc_\" + str(i) for i in range(len(normalized_corpus_news))]\n",
        "topic_names = [\"Topic_\" + str(i) for i in range(5)]\n",
        "\n",
        "#convert to dataframe\n",
        "df_document_topic = pd.DataFrame(np.round(lda_news_topic_weights, 5), columns=topic_names, index=doc_names)\n",
        "df_document_topic.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic_0</th>\n",
              "      <th>Topic_1</th>\n",
              "      <th>Topic_2</th>\n",
              "      <th>Topic_3</th>\n",
              "      <th>Topic_4</th>\n",
              "      <th>dominant_topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc_0</th>\n",
              "      <td>0.51723</td>\n",
              "      <td>0.00247</td>\n",
              "      <td>0.02920</td>\n",
              "      <td>0.43227</td>\n",
              "      <td>0.01883</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_1</th>\n",
              "      <td>0.52954</td>\n",
              "      <td>0.01627</td>\n",
              "      <td>0.01566</td>\n",
              "      <td>0.29845</td>\n",
              "      <td>0.14008</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_2</th>\n",
              "      <td>0.75160</td>\n",
              "      <td>0.00332</td>\n",
              "      <td>0.00345</td>\n",
              "      <td>0.17388</td>\n",
              "      <td>0.06775</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_3</th>\n",
              "      <td>0.57943</td>\n",
              "      <td>0.01716</td>\n",
              "      <td>0.01608</td>\n",
              "      <td>0.37172</td>\n",
              "      <td>0.01561</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_4</th>\n",
              "      <td>0.22183</td>\n",
              "      <td>0.14848</td>\n",
              "      <td>0.00753</td>\n",
              "      <td>0.20130</td>\n",
              "      <td>0.42085</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_5</th>\n",
              "      <td>0.19180</td>\n",
              "      <td>0.01456</td>\n",
              "      <td>0.01451</td>\n",
              "      <td>0.16501</td>\n",
              "      <td>0.61412</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_6</th>\n",
              "      <td>0.66917</td>\n",
              "      <td>0.00859</td>\n",
              "      <td>0.00848</td>\n",
              "      <td>0.17895</td>\n",
              "      <td>0.13482</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_7</th>\n",
              "      <td>0.52788</td>\n",
              "      <td>0.00320</td>\n",
              "      <td>0.05528</td>\n",
              "      <td>0.24934</td>\n",
              "      <td>0.16430</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_8</th>\n",
              "      <td>0.59171</td>\n",
              "      <td>0.00284</td>\n",
              "      <td>0.10940</td>\n",
              "      <td>0.29313</td>\n",
              "      <td>0.00292</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_9</th>\n",
              "      <td>0.40790</td>\n",
              "      <td>0.00073</td>\n",
              "      <td>0.17579</td>\n",
              "      <td>0.32029</td>\n",
              "      <td>0.09528</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_10</th>\n",
              "      <td>0.93163</td>\n",
              "      <td>0.01693</td>\n",
              "      <td>0.01662</td>\n",
              "      <td>0.01819</td>\n",
              "      <td>0.01663</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_11</th>\n",
              "      <td>0.01794</td>\n",
              "      <td>0.01823</td>\n",
              "      <td>0.01840</td>\n",
              "      <td>0.29181</td>\n",
              "      <td>0.65362</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_12</th>\n",
              "      <td>0.52685</td>\n",
              "      <td>0.00094</td>\n",
              "      <td>0.16374</td>\n",
              "      <td>0.30748</td>\n",
              "      <td>0.00099</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_13</th>\n",
              "      <td>0.98855</td>\n",
              "      <td>0.00281</td>\n",
              "      <td>0.00280</td>\n",
              "      <td>0.00301</td>\n",
              "      <td>0.00283</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc_14</th>\n",
              "      <td>0.02104</td>\n",
              "      <td>0.71531</td>\n",
              "      <td>0.02118</td>\n",
              "      <td>0.02065</td>\n",
              "      <td>0.22183</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Topic_0  Topic_1  Topic_2  Topic_3  Topic_4  dominant_topic\n",
              "Doc_0   0.51723  0.00247  0.02920  0.43227  0.01883               0\n",
              "Doc_1   0.52954  0.01627  0.01566  0.29845  0.14008               0\n",
              "Doc_2   0.75160  0.00332  0.00345  0.17388  0.06775               0\n",
              "Doc_3   0.57943  0.01716  0.01608  0.37172  0.01561               0\n",
              "Doc_4   0.22183  0.14848  0.00753  0.20130  0.42085               4\n",
              "Doc_5   0.19180  0.01456  0.01451  0.16501  0.61412               4\n",
              "Doc_6   0.66917  0.00859  0.00848  0.17895  0.13482               0\n",
              "Doc_7   0.52788  0.00320  0.05528  0.24934  0.16430               0\n",
              "Doc_8   0.59171  0.00284  0.10940  0.29313  0.00292               0\n",
              "Doc_9   0.40790  0.00073  0.17579  0.32029  0.09528               0\n",
              "Doc_10  0.93163  0.01693  0.01662  0.01819  0.01663               0\n",
              "Doc_11  0.01794  0.01823  0.01840  0.29181  0.65362               4\n",
              "Doc_12  0.52685  0.00094  0.16374  0.30748  0.00099               0\n",
              "Doc_13  0.98855  0.00281  0.00280  0.00301  0.00283               0\n",
              "Doc_14  0.02104  0.71531  0.02118  0.02065  0.22183               1"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#vector of indecies for columns with the highest value by each row in df_document_topic\n",
        "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "\n",
        "#add dominant_topic as a column to df_document_topic\n",
        "df_document_topic['dominant_topic'] = dominant_topic\n",
        "df_document_topic.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu2k3DAYFWWV"
      },
      "source": [
        "Now, let's normalize the corpus and create the Bag-of-Words representation of the data. We'll limit the number of features to **1000 most frequent features** to compute the topic model faster. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DcX252nFWWV"
      },
      "source": [
        "Now let's fit the topic model. We need to **set the number of topics** first. We are *lucky to know* that there are **4 topics** (atheism, religion, computer graphics, and space science) and it will allow us to judge the performance of the topic model better.\n",
        "\n",
        "**Note**: It will take a couple of minutes for the estimation to finish. The larger the number of iterations (max_iter) you allow for, the longer it takes. You can also find that you might get a slightly different result (for the estimated weights) if you rerun the model (qualitatively, the result is likely to stay the same). This behavior is due to the stochastic nature of the model. Estimation starts from a random guess for the weights and iteratively improves. When we do not allow for a large number of iterations, we are more likely to be affected by the random starting values. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3hRRCFDFWWV"
      },
      "source": [
        "Display results with top 10 words for each topic:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXuO7WLlFWWW"
      },
      "source": [
        "Display **word vectors** (words are in alphabetical order) for each topic. Each column is a topic:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxOqncqyFWWW"
      },
      "source": [
        "Now, **sort by word weights in Topic 0** (descending order) and see the weights by 10 most frequent words in Topic 0:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLpKX958FWWX"
      },
      "source": [
        "### **Topic Model Visualization**\n",
        "\n",
        "You can **visualize** the topics: topic size, frequency of words in a topic and so on.\n",
        "\n",
        "In this visualization, you can rank words in a topic by **relevancy**: do you want rare and exclusive terms (i.e. found mostly in that topic) or terms that are used frequently in that topic, but not always exclusive to that topic? Relevancy parameter is λ (0 ≤ λ ≤ 1). You can adjust it:\n",
        "\n",
        "* small λ highlights potentially rare, but exclusive terms for the selected topic;\n",
        "* large values of λ (near 1) highlight frequent, but not necessarily exclusive, terms for the selected topic;\n",
        "\n",
        "Relevancy is measured as: \n",
        "\n",
        "    Relevancy = λ log[p(term | topic)] + (1 - λ) log[p(term | topic)/p(term)], \n",
        "   \n",
        "   where p(term | topic) stands for word weight in a topic and p(term) stands for word's weight in a corpus.\n",
        "\n",
        "Additional information on how to use this visualization:\n",
        "* http://www.kennyshirley.com/LDAvis/\n",
        "* https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf\n",
        "\n",
        "We installed all the **pyLDAvis** module required for this visualization in Session Prep. Now let's use it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuwkTzh9FWWZ"
      },
      "source": [
        "### **How To Find Dominant Topic in a Document**\n",
        "\n",
        "Each document contains several topics. One of the topics is **dominant**, i.e. it is the largest topic in the document. That topic gives you an answer to the question: **What is this document about?** In other words, the document's dominant topic **summarizes** the document. \n",
        "\n",
        "Let's assign a dominant topic to **each document** in our corpus. Weights in a word vector for a topic provide a measure of association for the word with the topic. If you sum weights for a particular topic across all words in a document, you'll get the weight of that topic in the document.\n",
        "\n",
        "The attribute **.transform** to our function **lda_news** computes the weights of each topic in documents: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IVtPDPtQFWWZ"
      },
      "outputs": [],
      "source": [
        "lda_news_topic_weights = lda_news.transform(bow_news_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxFB8PelFWWZ"
      },
      "source": [
        "Let's convert lda_news_topic_weights into a nice-looking dataframe and have a look at the computed topic weights in documents:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCP5EM4rFWWa"
      },
      "source": [
        "The topic with the highest weight in each document is a **dominant topic**. The weights across the 4 topics sum up to 1. Let's add a column that shows dominant topic for each document:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut6yrr_pFWWa"
      },
      "source": [
        "### **Topic Model Evaluation: Log-likelihood, Perplexity and Coherence Scores**\n",
        "\n",
        "Log-likelihood, Perplexity and Coherence Score are **measures of performance** for a topic model. They are used for comparing and discriminating between topic models estimated on the same data. Log-likelihood, perplexity and coherence scores **do not have** a baseline or a threshold values and therefore are useful only for comparing models. \n",
        "\n",
        "How do you specify different models? You can set **different number of topics** and also play with the **parameters of the Dirichlet distributions**. \n",
        "\n",
        "#### **Coherence Score**\n",
        "\n",
        "We will use a function **CoherenceModel()** from the **gensim** module (you can also explore that package as it can be used to estimate an LDA model). The sklearn module does not have the functionality to compute the coherence score. Let's install the gensim package and the functions needed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "t1tKRdn0FWWa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.8/site-packages (4.1.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from gensim) (5.2.1)\n"
          ]
        }
      ],
      "source": [
        "!{sys.executable} -m pip install gensim\n",
        "import gensim\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVyUhxinFWWa"
      },
      "source": [
        "The function CoherenceModel() needs as **inputs**:\n",
        "\n",
        "**1. Dictionary of the corpus**<br>\n",
        "**2. Corpus with each document represented as Bag-of-Words**<br>\n",
        "**3. An array of top words for each topic: we'll have top 20 words for each topic** \n",
        "  \n",
        "We will now create those objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MSV3X0vVFWWb"
      },
      "outputs": [],
      "source": [
        "#tokenizing the corpus\n",
        "news_corpus_tokenized = [tokenize_text(normalized_corpus_news[doc_id]) for doc_id in range(len(normalized_corpus_news))]\n",
        "\n",
        "#Dictionary of the corpus:\n",
        "news_dictionary = Dictionary(news_corpus_tokenized)\n",
        "\n",
        "#Bag-of-words representation for each document of the corpus:\n",
        "news_corpus_bow = [news_dictionary.doc2bow(doc) for doc in news_corpus_tokenized]\n",
        "\n",
        "#top 20 words for each topic (using the function defined in session prep)\n",
        "topic_topwords = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news, n_words=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laMIgV5NFWWb"
      },
      "source": [
        "Now let's compute **the coherence score for the model overall**. We use one of the coherence metrics \"u-mass\" which measures semantic similarity of words in a topic, but there are other metrics as well.\n",
        "\n",
        "*Note: You can check out different coherence metrics here if you are interested: https://dl.acm.org/doi/abs/10.1145/2684822.2685324*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9_G-6nDLFWWb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence score for the model:  -1.5086\n"
          ]
        }
      ],
      "source": [
        "cm = CoherenceModel(topics=topic_topwords, \n",
        "                    corpus = news_corpus_bow , \n",
        "                    dictionary = news_dictionary, coherence='u_mass')\n",
        "print(\"Coherence score for the model: \", np.round(cm.get_coherence(), 4))  # get coherence value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55VFwUJNFWWb"
      },
      "source": [
        "You can also see **coherence scores by topic**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uEjEYpOXFWWb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence score by topic (higher values are better):  [-1.5913 -1.6719 -1.3431 -1.4282]\n"
          ]
        }
      ],
      "source": [
        "print(\"Coherence score by topic (higher values are better): \", np.round(cm.get_coherence_per_topic(),4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7re4HnVFWWc"
      },
      "source": [
        "#### **Log-Likelihood Score**\n",
        "\n",
        "To compute the log-likelihood score we use the **.score** attribute of our defined and fitted LDA function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "KFGsNF8pFWWc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log-Likelihood (higher values are better):  -741444.8353556208\n"
          ]
        }
      ],
      "source": [
        "print(\"Log-Likelihood (higher values are better): \", lda_news.score(bow_news_corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtqJ4d5bFWWc"
      },
      "source": [
        "#### **Perplexity Score**\n",
        "\n",
        "To compute the Perplexity score we use the **.perplexity** attribute of our defined and fitted LDA function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "A9AnAADnFWWc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity (lower values are better):  575.2087781096856\n"
          ]
        }
      ],
      "source": [
        "print(\"Perplexity (lower values are better): \", lda_news.perplexity(bow_news_corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3oKlL9zFWWd"
      },
      "source": [
        "The coherence scores by topic are [-1.5466 -1.3606 -1.4048 -1.7395]. We got coherent topics as judged by reading through the top words of each topic: we see that words are related to each other and we can use them to tell a story easily, without thinking too hard of how to include each of the top words into a story. Topic with coherence score of -1.3606 (the highest number) is of the highest quality (most coherent and likely easier for a human to inperprete). Having one topic with a drastically different coherence score might indicate that we can probably do better with a smaller number of topics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-reNHSXFWWd"
      },
      "source": [
        "**Answer 3.2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V83CsMLFWWd"
      },
      "source": [
        "<font color=blue>Code (complete the lines):<font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2JIca03TFWWd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log-Likelihood (higher values are better):  -743489.8926210115\n",
            "Perplexity (lower values are better):  585.3797180350961\n",
            "Coherence score for the model: (higher values are better) -1.44\n"
          ]
        }
      ],
      "source": [
        "#Log-Likelihood\n",
        "print(\"Log-Likelihood (higher values are better): \", lda_news_3_topics.score(bow_news_corpus))\n",
        "\n",
        "#Perplexity score:\n",
        "print(\"Perplexity (lower values are better): \", lda_news_3_topics.perplexity(bow_news_corpus))\n",
        "\n",
        "#coherence score for 3 topics:\n",
        "topic_topwords_3_topics = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news_3_topics, n_words=20)\n",
        "cm_3_topics = CoherenceModel(topics=topic_topwords_3_topics, \n",
        "                             corpus = news_corpus_bow, \n",
        "                             dictionary = news_dictionary, coherence='u_mass')\n",
        "#overall coherence score for the model:\n",
        "print(\"Coherence score for the model: (higher values are better)\", np.round(cm_3_topics.get_coherence(), 3)) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuT8Py9_FWWd"
      },
      "source": [
        "<font color=blue>Discussion:<br><br><font>\n",
        "    When we compare a model with 3 topics to the model with 4 topics using the stats (output above), we get an inconsistent picture: <br><font>\n",
        "    \n",
        "<font color=blue> - log-likelihood is lower for the model with 3 topics, which means 4 topics is better (3 topics: -743309.96 < 4 topics: -741363.0) <br><font>\n",
        "<font color=blue>- perplexity score is higher for 3 topics, which means 4 topics is better (3 topics: 584.47 > 4 topics:  574.8)<br><font>\n",
        "<font color=blue>- coherence score is higher for the model with 3 topics, which means 3 topics is better (3 topics: -1.436 > 4 topics: -1.5129)<br><font>\n",
        "    \n",
        "<font color=blue>Thus, coherence score is in favor of a 3-topic model, while the other two stats are in favor of a 4-topic model. \n",
        "    \n",
        "<font color=blue>From experimental research, it is known that coherence score typically reflects human judgment better, so we might want to rely on coherence score more than on the other two stats. Given that in the 3-topics model, atheism and religion were combined (atheism is often viewed as a discussion of religion) we, as humans, might agree with the coherence score stat here. <font>\n",
        "    \n",
        "<font color=blue>You may have noticed that in the LDA model with 4 topics, the topic with top words \"think\", \"like\", \"know\", \"people\" and so on (the \"atheism\" topic) was a bit harder to interpret than other 3 topics. The LDA model allocated to this topic the top words that can be interpreted as related to doubt and knowledge which are important ideas in the atheism discourse. For this reason, some humans may prefer a model with 4 topics (vs. 3 topics) as the atheism topic is subtly different from the overall discussion of religion. <font>\n",
        "\n",
        "<font color=blue>**NOTE:** that estimation results can be slightly different for different runs of the models. The output can be sensitive to the starting values of the LDA algorithm. The reason is that in the lab script we set the number of iteration (max_iter) to a relatively small number to speed up the estimation.<font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvceJzzaFWWd"
      },
      "source": [
        "**Answer 3.3:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIlQaRTjFWWd"
      },
      "source": [
        "Code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZsUDllIMFWWd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log-Likelihood (higher values are better):  -751871.0460985524\n",
            "Perplexity (lower values are better):  628.9762131689758\n",
            "Coherence score for the model: (higher values are better) -1.421\n"
          ]
        }
      ],
      "source": [
        "  #fit LDA with 2 topics:\n",
        "lda_news_2_topics = LatentDirichletAllocation(n_components=2, max_iter=100,\n",
        "                                              doc_topic_prior = 0.25,\n",
        "                                              topic_word_prior = 0.25).fit(bow_news_corpus)\n",
        "\n",
        "#Log-Likelihood\n",
        "print(\"Log-Likelihood (higher values are better): \", lda_news_2_topics.score(bow_news_corpus))\n",
        "\n",
        "#Perplexity score:\n",
        "print(\"Perplexity (lower values are better): \", lda_news_2_topics.perplexity(bow_news_corpus))\n",
        "\n",
        "#coherence score for 3 topics:\n",
        "topic_topwords_2_topics = get_topic_words(vectorizer = bow_vectorizer_news, lda_model = lda_news_2_topics, n_words=20)\n",
        "cm_2_topics = CoherenceModel(topics=topic_topwords_2_topics, \n",
        "                             corpus = news_corpus_bow, \n",
        "                             dictionary = news_dictionary, coherence='u_mass')\n",
        "#overall coherence score for the model:\n",
        "print(\"Coherence score for the model: (higher values are better)\", np.round(cm_2_topics.get_coherence(), 3))  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV2uohscFWWe"
      },
      "source": [
        "<font color=blue>Discussion:<font>\n",
        "\n",
        "All stats consistently show that the model with 2 topics is worse than the model with 3 topics or 4 topics. In the model with 2 topics, the atheism and religion classes and the space science and computer graphics classes were combined, respectively. Many people would agree that space science and computer graphics (although both technical topics) should not be combined. Therefore, a model with 2 topics is definitely worse than a model with 3 or 4 topics. The stats and human judgment agree here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15az3h_rFWWe"
      },
      "source": [
        "<br>**NOTE:** Generally, you can write a simple script that selects the best topic model **automatically** based on a criterion for \"best model\" (log-likelihood, perplexity, or coherence score). The script can vary both parameters of the Dirichlet distributions and the number of topics, or just the number of topics."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab_4_Topic_Modeling_SP2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
